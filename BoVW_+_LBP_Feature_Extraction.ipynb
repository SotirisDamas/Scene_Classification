{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8t_TuY--_3Gd"
      },
      "outputs": [],
      "source": [
        "import os, random, pickle, joblib, time, gc, zipfile\n",
        "import numpy as np\n",
        "import cv2\n",
        "from skimage.feature import local_binary_pattern\n",
        "from torchvision.datasets import ImageFolder\n",
        "from sklearn.cluster import MiniBatchKMeans\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from google.colab import drive\n",
        "\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_recall_fscore_support, confusion_matrix,\n",
        "    classification_report, ConfusionMatrixDisplay)\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_ROOT = \"/content\"\n",
        "SAVE_PATH = \"/content/drive/MyDrive/COMP6721/scene_bovw_lbp_features_NF200_V600.npz\"\n",
        "\n",
        "#  (VOCAB_SIZE=600, ORB_NFEATURES=200, LBP_Radii=[1,2])\n",
        "ORB_NFEATURES = 200    #300            # max keypoints per image\n",
        "VOCAB_SIZE   = 600     #100            # visual words\n",
        "LBP_RADIUS   = [1, 2]                  # (P = 8*R) uniform LBP"
      ],
      "metadata": {
        "id": "4-80ettPAJJS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_NNI6HCjBeDc",
        "outputId": "531274c6-abde-40b5-aae5-ee3fb3513285"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "zip_path = \"/content/drive/My Drive/dataset/Comp6721_Project_Dataset.zip\"\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall('/content/')"
      ],
      "metadata": {
        "id": "g7IDknSrBSzh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset_full = ImageFolder(os.path.join(DATA_ROOT, \"Training\"))\n",
        "test_dataset       = ImageFolder(os.path.join(DATA_ROOT, \"Test\"))\n",
        "\n",
        "train_paths = [p for p, _ in train_dataset_full.samples]\n",
        "train_lbls  = [lbl for _, lbl in train_dataset_full.samples]\n",
        "\n",
        "test_paths  = [p for p, _ in test_dataset.samples]\n",
        "test_lbls   = [lbl for _, lbl in test_dataset.samples]\n",
        "\n",
        "classes = train_dataset_full.classes\n",
        "print(\"Classes:\", classes)\n",
        "print(f\"Number of training images: {len(train_dataset_full)}\")\n",
        "print(f\"Number of test images: {len(test_dataset)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ok3QCsQ4A368",
        "outputId": "bd98dc10-2ef2-4c5f-d388-2f50582fd395"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classes: ['library-indoor', 'museum-indoor', 'shopping_mall-indoor']\n",
            "Number of training images: 15000\n",
            "Number of test images: 300\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "idxs = np.arange(len(train_paths))\n",
        "train_idx, val_idx = train_test_split(\n",
        "    idxs, test_size=0.10, stratify=train_lbls, random_state=42)\n",
        "\n",
        "train_paths_split = [train_paths[i] for i in train_idx]\n",
        "train_lbls_split  = [train_lbls[i]  for i in train_idx]\n",
        "val_paths_split   = [train_paths[i] for i in val_idx]\n",
        "val_lbls_split    = [train_lbls[i]  for i in val_idx]\n",
        "\n",
        "print(f\"Train images: {len(train_paths_split)},  Val images: {len(val_paths_split)}\")\n",
        "\n",
        "# LabelEncoder → 0,1,2\n",
        "le = LabelEncoder().fit(train_lbls)\n",
        "y_train = le.transform(train_lbls_split)\n",
        "y_val   = le.transform(val_lbls_split)\n",
        "y_test  = le.transform(test_lbls)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-8KMgfplD0Kr",
        "outputId": "b65c7962-cccd-459d-af73-bf0c80e3af4b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train images: 13500,  Val images: 1500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Learn 600-word ORB vocabulary  \n",
        "\n",
        "* Streams ORB descriptors from the **train** split  \n",
        "* Fits `MiniBatchKMeans` (partial-fit batches of 10 k descriptors)"
      ],
      "metadata": {
        "id": "VAB9lZNUo1wu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  Build ORB → MiniBatchKMeans vocabulary on train imgs\n",
        "orb = cv2.ORB_create(nfeatures=ORB_NFEATURES)\n",
        "kmeans = MiniBatchKMeans(n_clusters=VOCAB_SIZE,\n",
        "                         batch_size=10_000,     # descriptors / partial_fit\n",
        "                         random_state=42,\n",
        "                         verbose=0)\n",
        "\n",
        "def stream_orb_descriptors(paths, batch=500):\n",
        "    \"\"\"Yield lists of descriptors in chunks to avoid RAM blow-up.\"\"\"\n",
        "    buff = []\n",
        "    for i, p in enumerate(paths):\n",
        "        img = cv2.imread(p, cv2.IMREAD_GRAYSCALE)\n",
        "        if img is None:\n",
        "            continue\n",
        "        _, des = orb.detectAndCompute(img, None)\n",
        "        if des is not None:\n",
        "            if des.shape[0] > ORB_NFEATURES:\n",
        "                des = des[:ORB_NFEATURES]\n",
        "            buff.append(des.astype(np.float32))\n",
        "        if (i + 1) % batch == 0 and buff:\n",
        "            yield np.vstack(buff)\n",
        "            buff = []\n",
        "    if buff:\n",
        "        yield np.vstack(buff)\n",
        "\n",
        "print(\"Fitting MiniBatchKMeans vocabulary …\")\n",
        "t0 = time.time()\n",
        "for desc_batch in stream_orb_descriptors(train_paths_split):\n",
        "    kmeans.partial_fit(desc_batch)\n",
        "print(f\"Done in {time.time()-t0:.1f}s\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HQ_1bZkQF9t2",
        "outputId": "27b1c4dd-82d0-4ccc-9e1b-d4b8ba44aa96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting MiniBatchKMeans vocabulary …\n",
            "Done in 34.6s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Extract BoVW (600) + LBP (28) descriptors  \n",
        "\n",
        "* **BoVW:** assign each ORB descriptor to nearest visual word → 600-bin hist  \n",
        "* **LBP:** uniform patterns, radii 1 & 2 → 10 + 18 bins  \n",
        "* Concatenate → 628-dim feature per image  \n",
        "* Save train / val / test matrices → `scene_bovw_lbp_features_NF200_V600.npz`"
      ],
      "metadata": {
        "id": "tP5aoIX6pBTf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# extract BoVW + LBP\n",
        "n_lbp_bins = sum((r * 8 + 2) for r in LBP_RADIUS)\n",
        "\n",
        "def extract_features(path):\n",
        "    # read + grayscale\n",
        "    img = cv2.imread(path)\n",
        "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # ORB → visual words hist\n",
        "    kp, des = orb.detectAndCompute(gray, None)\n",
        "    bovw_hist = np.zeros(VOCAB_SIZE, dtype=np.float32)\n",
        "    if des is not None and des.size:\n",
        "        if des.shape[0] > ORB_NFEATURES:\n",
        "            des = des[:ORB_NFEATURES]\n",
        "        words = kmeans.predict(des.astype(np.float32))\n",
        "        bovw_hist, _ = np.histogram(words, bins=np.arange(VOCAB_SIZE+1))\n",
        "        if bovw_hist.sum():\n",
        "            bovw_hist = bovw_hist / bovw_hist.sum()\n",
        "\n",
        "    # LBP hist\n",
        "    lbp_hists = []\n",
        "    # Iterate through each radius in LBP_RADIUS\n",
        "    for r in LBP_RADIUS:\n",
        "        # Calculate P for the current radius\n",
        "        p = r * 8\n",
        "        lbp = local_binary_pattern(gray, P=p, R=r, method='uniform')\n",
        "        # Calculate histogram for the current LBP result\n",
        "        max_bin = p + 2 # Uniform LBP has P+2 possible values (P + 2 for non-uniform)\n",
        "        hist, _ = np.histogram(lbp.ravel(), bins=max_bin, range=(0, max_bin))\n",
        "        if hist.sum():\n",
        "             hist = hist / hist.sum()\n",
        "        lbp_hists.append(hist)\n",
        "\n",
        "    # Concatenate the LBP histograms from different radii\n",
        "    lbp_hist = np.concatenate(lbp_hists)\n",
        "\n",
        "    # concat\n",
        "    # Recalculate FEAT_LEN based on the new LBP histogram length\n",
        "    feat_len = VOCAB_SIZE + len(lbp_hist)\n",
        "    return np.concatenate([bovw_hist, lbp_hist]).astype(np.float32)\n",
        "\n",
        "# Re-calculate FEAT_LEN based on the individual LBP histograms lengths\n",
        "# Need to call extract_features once to get the correct length of the concatenated LBP hist\n",
        "temp_features = extract_features(train_paths_split[0])\n",
        "FEAT_LEN = len(temp_features)\n",
        "\n",
        "print(\"Final feature length:\", FEAT_LEN)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LRZcBGXaGqc2",
        "outputId": "4fb0239d-2ff3-4ef6-9b84-30936fc587b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final feature length: 628\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def build_matrix(paths):\n",
        "    X = np.zeros((len(paths), FEAT_LEN), dtype=np.float32)\n",
        "    for i, p in enumerate(paths):\n",
        "        X[i] = extract_features(p)\n",
        "    return X\n",
        "\n",
        "X_train = build_matrix(train_paths_split)\n",
        "print(\"Training feature matrix built.\")\n",
        "X_val   = build_matrix(val_paths_split)\n",
        "print(\"Validation feature matrix built.\")\n",
        "X_test  = build_matrix(test_paths)\n",
        "print(\"Test feature matrix built.\")\n",
        "\n",
        "# Free a bit of RAM\n",
        "gc.collect();"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CPyutistHJDe",
        "outputId": "0eee4113-9afa-489e-9ca8-a0a7a20dfa7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training feature matrix built.\n",
            "Validation feature matrix built.\n",
            "Test feature matrix built.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Feature shapes:\", X_train.shape, X_val.shape, X_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XzAEF1_UPzxC",
        "outputId": "a504b73b-d6de-473b-a544-d1a0ba8f7d58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature shapes: (13500, 628) (1500, 628) (300, 628)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Save everything to Google Drive for quick reload\n",
        "np.savez_compressed(\n",
        "    SAVE_PATH,\n",
        "    X_train=X_train, y_train=y_train,\n",
        "    X_val=X_val,     y_val=y_val,\n",
        "    X_test=X_test,   y_test=y_test,\n",
        "    classes=np.array(classes)\n",
        ")\n",
        "print(f\"Saved to {SAVE_PATH}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5k0fzFWyHyI4",
        "outputId": "047c4a3a-1cde-48cf-bf47-83d3532891d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved to /content/drive/MyDrive/COMP6721/scene_bovw_lbp_features_NF200_V600.npz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "joblib.dump(kmeans,\"/content/drive/MyDrive/COMP6721/models/vocab_kmeans_600.joblib\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "392KlKIo8TLv",
        "outputId": "0cad86d4-725a-4026-9a71-4e3440276d1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/drive/MyDrive/COMP6721/models/vocab_kmeans_600.joblib']"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "# Data Re-loading\n",
        "data = np.load(SAVE_PATH, allow_pickle=True)\n",
        "X_train, y_train = data['X_train'], data['y_train']\n",
        "X_val,   y_val   = data['X_val'],   data['y_val']\n",
        "X_test,  y_test  = data['X_test'],  data['y_test']\n",
        "classes = data['classes']\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "uIpKAl5rIMbr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Single-Run Random Forest"
      ],
      "metadata": {
        "id": "JDaEyp3cqLMG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rf = RandomForestClassifier(max_depth=20,\n",
        "                            min_samples_leaf=10,\n",
        "                            max_features='sqrt',\n",
        "                            n_estimators=200,\n",
        "                            bootstrap=True,\n",
        "                            max_samples=0.7,\n",
        "                            random_state=42,\n",
        "                            n_jobs=-1)\n",
        "print(f\"leaf={10} → VAL = {rf.fit(X_train, y_train).score(X_val, y_val):.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KyL4U132MEvm",
        "outputId": "60489e99-973f-4684-ca66-a6dda11bbf85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "leaf=10 → VAL = 0.597\n"
          ]
        }
      ]
    }
  ]
}